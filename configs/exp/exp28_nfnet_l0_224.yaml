# @package exp

name: exp28_nfnet_l0_224
input:
  size:
    - 224
    - 224
  use_pseudo: true
model:
  seed: 0
  backbone: nfnet_l0
  pretrained: true
  do_dropout: false
  hidden_dim: null
training:
  batch_size : 128
  epochs: 91  # 5.5 cycles (8 * 2 * 5.5) + 3 epochs
  learning_rate: 1e-3
  optimizer: adamw
  weight_decay: 1e-2
  verbose_epochs: 10
  log_every_n_steps: 10  # default=50
  criterion: 
    loss_function: LabelSmoothingCrossEntropy
    smoothing: 0.1
lrate_scheduler:
  scheduler: LinearWarmupCosineAnnealingLR
  T_max: 272  # 8 epochs x 34 mini-batches (4420 // 128)
  warmup_epochs: 102  # 3 epochs x 34 batches
  warmup_start_lr: 0.0
  eta_min: 0.0
  m_mul: 0.8
tta:
  batch_size : 128
  size: 291  # 224 * 1.3